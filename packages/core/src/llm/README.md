# LLM Client ä½¿ç”¨æŒ‡å—

åŸºäº Vercel AI SDK çš„ç»Ÿä¸€ LLM å®¢æˆ·ç«¯ï¼Œæä¾›ç®€æ´çš„ API è°ƒç”¨æ¥å£ã€‚

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [åˆå§‹åŒ–æ–¹å¼](#åˆå§‹åŒ–æ–¹å¼)
- [åŸºç¡€ç”¨æ³•](#åŸºç¡€ç”¨æ³•)
- [æ¨ç†æ¨¡å‹é…ç½®](#æ¨ç†æ¨¡å‹é…ç½®)
- [å·¥å…·è°ƒç”¨](#å·¥å…·è°ƒç”¨)
- [æµå¼è¾“å‡º](#æµå¼è¾“å‡º)
- [API å‚è€ƒ](#api-å‚è€ƒ)

---

## å¿«é€Ÿå¼€å§‹

```typescript
import { LLMClient } from '@monkey-agent/core';

// åˆ›å»ºå®¢æˆ·ç«¯
const client = new LLMClient({
  provider: 'openai',
  apiKey: 'sk-...',
  model: 'gpt-4',
});

// å‘é€æ¶ˆæ¯
const response = await client.chat([
  { role: 'user', content: 'Hello!' }
]);

console.log(response.text);
```

---

## åˆå§‹åŒ–æ–¹å¼

### æ–¹å¼ 1: æ ‡å‡†é…ç½®ï¼ˆæ¨èï¼‰

```typescript
const client = new LLMClient({
  provider: 'openai',     // 'openai' | 'anthropic' | 'google' | 'openrouter'
  apiKey: 'sk-...',
  model: 'gpt-4',
  temperature: 0.7,
  maxTokens: 2000,
});
```

**æ”¯æŒçš„ Providerï¼š**

| Provider | è¯´æ˜ | ç¤ºä¾‹æ¨¡å‹ |
|---------|------|---------|
| `openai` | OpenAI å®˜æ–¹ API | `gpt-4`, `gpt-4o`, `gpt-3.5-turbo` |
| `anthropic` | Anthropic å®˜æ–¹ API | `claude-3-5-sonnet`, `claude-3-opus` |
| `google` | Google AI Studio | `gemini-2.0-flash-exp`, `gemini-1.5-pro` |
| `openrouter` | OpenRouter ç»Ÿä¸€æ¥å£ | `openai/gpt-4o`, `anthropic/claude-3.5-sonnet` |
| `bedrock` | Amazon Bedrock | `anthropic.claude-3-5-sonnet`, `meta.llama3-70b` |
| `azure` | Azure OpenAI | `gpt-4o` (éƒ¨ç½²åç§°) |
| `vertex` | Google Vertex AI | `gemini-1.5-pro`, `gemini-2.0-flash` |
| `deepseek` | DeepSeek API | `deepseek-chat`, `deepseek-reasoner` |

> ğŸ“š è¯¦ç»†çš„æ–°æä¾›å•†ä½¿ç”¨æŒ‡å—è¯·æŸ¥çœ‹ [NEW_PROVIDERS.md](./NEW_PROVIDERS.md)

### æ–¹å¼ 2: ä¼ å…¥ LanguageModelï¼ˆæœ€çµæ´»ï¼‰

```typescript
import { createOpenAI } from '@ai-sdk/openai';

const openai = createOpenAI({
  apiKey: 'sk-...',
  baseURL: 'https://custom-endpoint.com',
});

const client = new LLMClient({
  languageModel: openai('gpt-4'),
  temperature: 0.7,
});
```

---

## åŸºç¡€ç”¨æ³•

### æ™®é€šå¯¹è¯

```typescript
const response = await client.chat([
  { role: 'system', content: 'You are a helpful assistant.' },
  { role: 'user', content: 'What is the capital of France?' },
]);

// chat() ç›´æ¥è¿”å› AI SDK çš„ GenerateTextResult
console.log(response.text);        // å“åº”æ–‡æœ¬
console.log(response.usage);       // Token ä½¿ç”¨ç»Ÿè®¡
console.log(response.finishReason); // ç»“æŸåŸå› 
console.log(response.toolCalls);    // å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
```

### è°ƒç”¨é€‰é¡¹

```typescript
const response = await client.chat(messages, {
  system: 'You are a helpful assistant.',
  temperature: 0.8,
  maxTokens: 1000,
  topP: 0.9,
  presencePenalty: 0.5,
  frequencyPenalty: 0.5,
  stopSequences: ['\n\n'],
  seed: 42,
});
```

---

## æ¨ç†æ¨¡å‹é…ç½®

### OpenAI o1 ç³»åˆ—

```typescript
const o1Client = new LLMClient({
  provider: 'openai',
  apiKey: 'sk-...',
  model: 'o1-preview',
  reasoning: { 
    effort: 'high'  // 'low' | 'medium' | 'high'
  }
});

const response = await o1Client.chat([
  { role: 'user', content: 'Solve this complex problem...' }
]);

// è®¿é—®æ¨ç†ä¿¡æ¯
console.log(response.reasoning);                // æ¨ç†å†…å®¹
console.log(response.usage.reasoningTokens);    // æ¨ç† token æ•°

// è°ƒç”¨æ—¶è¦†ç›–é…ç½®
const result = await o1Client.chat(messages, {
  reasoning: { effort: 'high' }
});
```

### Claude Extended Thinking

```typescript
const claudeClient = new LLMClient({
  provider: 'anthropic',
  apiKey: 'sk-ant-...',
  model: 'claude-sonnet-4-5-20250929',
  reasoning: {
    thinking: 5000  // Token é¢„ç®—
    // æˆ– thinking: true (è‡ªåŠ¨æ¨¡å¼)
  }
});

const response = await claudeClient.chat(messages);
console.log(response.reasoningText);    // æ¨ç†æ–‡æœ¬
console.log(response.reasoningDetails); // æ¨ç†è¯¦æƒ…
```

### DeepSeek-R1ï¼ˆæ ‡ç­¾æå–ï¼‰

```typescript
import { createAzure } from '@ai-sdk/azure';

const azure = createAzure({ 
  resourceName: '...', 
  apiKey: '...' 
});

const deepseekClient = new LLMClient({
  languageModel: azure('deepseek-r1'),
  reasoning: { 
    tagName: 'think'  // æå– <think>...</think> æ ‡ç­¾
  }
});

const response = await deepseekClient.chat(messages);
console.log(response.reasoning);      // æå–çš„æ¨ç†å†…å®¹
```

---

## å·¥å…·è°ƒç”¨

### å®šä¹‰å’Œä½¿ç”¨å·¥å…·

```typescript
import { tool, z } from 'ai';

// 1. å®šä¹‰å·¥å…·
const weatherTool = tool({
  description: 'Get the current weather',
  parameters: z.object({
    location: z.string().describe('City name'),
  }),
  execute: async ({ location }) => {
    return { location, temp: 22, conditions: 'Sunny' };
  },
});

const tools = {
  getWeather: weatherTool,
};

// 2. è°ƒç”¨ LLM
const messages = [
  { role: 'user', content: 'What is the weather in Paris?' }
];

const response = await client.chat(messages, {
  tools,
  toolChoice: 'auto', // 'auto' | 'required' | 'none'
});

// 3. æ‰‹åŠ¨æ‰§è¡Œå·¥å…·
if (response.toolCalls) {
  for (const toolCall of response.toolCalls) {
    const toolResult = await tools[toolCall.toolName].execute(toolCall.input);
    const toolMessage = client.buildToolResultMessage(toolCall, toolResult);
    messages.push(toolMessage);
  }
  
  // 4. ç»§ç»­å¯¹è¯
  const nextResponse = await client.chat(messages);
  console.log(nextResponse.text);
}
```

### è¾…åŠ©æ–¹æ³•

```typescript
// æ„å»ºåŠ©æ‰‹æ¶ˆæ¯
const assistantMessage = client.buildAssistantMessage(response.toolCalls);

// æ„å»ºå·¥å…·ç»“æœæ¶ˆæ¯
const toolMessage = client.buildToolResultMessage(toolCall, result);

// æ„å»ºé”™è¯¯æ¶ˆæ¯
const errorMessage = client.buildToolResultMessage(
  toolCall, 
  { error: 'Tool execution failed' },
  true  // isError
);
```

---

## æµå¼è¾“å‡º

### æ–¹å¼ 1: çº¯æ–‡æœ¬æµï¼ˆæœ€å¸¸ç”¨ï¼‰

```typescript
const result = client.stream(messages, { tools });

for await (const text of result.textStream) {
  process.stdout.write(text);
}
```

### æ–¹å¼ 2: å®Œæ•´äº‹ä»¶æµ

```typescript
const result = client.stream(messages, { tools });

for await (const event of result.fullStream) {
  switch (event.type) {
    case 'text-delta':
      console.log('Text:', event.textDelta);
      break;
    case 'tool-call':
      console.log('Calling tool:', event.toolName, event.input);
      break;
    case 'tool-result':
      console.log('Tool result:', event.result);
      break;
  }
}
```

### æ–¹å¼ 3: ç­‰å¾…æœ€ç»ˆç»“æœ

```typescript
const result = client.stream(messages);

const finalText = await result.text;
const usage = await result.usage;
const finishReason = await result.finishReason;
```

### ä¾¿æ·æ–¹æ³•ï¼šstreamText

```typescript
for await (const text of client.streamText(messages)) {
  process.stdout.write(text);
}
```

---

## API å‚è€ƒ

### LLMConfig

```typescript
interface LLMConfig {
  // æ–¹å¼ 1: æ ‡å‡†é…ç½®
  provider?: 'openai' | 'anthropic' | 'google';
  apiKey?: string;
  model?: string;
  baseURL?: string;
  
  // æ–¹å¼ 2: ç›´æ¥ä¼ å…¥ LanguageModelï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
  languageModel?: LanguageModel;
  
  // é€šç”¨å‚æ•°
  temperature?: number;
  maxTokens?: number;
  
  // æ¨ç†é…ç½®
  reasoning?: ReasoningConfig;
}
```

### ReasoningConfig

```typescript
interface ReasoningConfig {
  disabled?: boolean;  // ç¦ç”¨æ¨ç†
  
  // OpenAI o1 ç³»åˆ—
  effort?: 'low' | 'medium' | 'high';
  
  // Claude Extended Thinking
  thinking?: boolean | number;  // true/false æˆ– token é¢„ç®—
  
  // DeepSeek-R1 ç­‰ï¼ˆæ ‡ç­¾æå–ï¼‰
  tagName?: string;  // å¦‚ 'think'
}
```

### LLMCallOptions

```typescript
interface LLMCallOptions<TOOLS extends ToolSet> {
  // ç³»ç»Ÿæç¤º
  system?: string;
  
  // ç”Ÿæˆå‚æ•°
  temperature?: number;
  maxTokens?: number;
  topP?: number;
  topK?: number;
  presencePenalty?: number;
  frequencyPenalty?: number;
  stopSequences?: string[];
  seed?: number;
  
  // å·¥å…·ç›¸å…³
  tools?: TOOLS;
  toolChoice?: 'auto' | 'required' | 'none' | { type: 'tool'; toolName: string };
  activeTools?: string[];
  
  // æ¨ç†é…ç½®ï¼ˆè¦†ç›–åˆå§‹åŒ–é…ç½®ï¼‰
  reasoning?: ReasoningConfig;
  
  // é«˜çº§å‚æ•°
  maxRetries?: number;
  abortSignal?: AbortSignal;
  headers?: Record<string, string>;
}
```

### LLMChatResult

`LLMChatResult` æ˜¯ AI SDK `GenerateTextResult` çš„ç±»å‹åˆ«åï¼š

```typescript
interface GenerateTextResult<TOOLS extends ToolSet> {
  // åŸºç¡€å†…å®¹
  text: string;                // ç”Ÿæˆçš„æ–‡æœ¬
  content: Array<any>;         // å®Œæ•´å†…å®¹æ•°ç»„
  
  // å·¥å…·è°ƒç”¨
  toolCalls?: Array<{
    toolCallId: string;
    toolName: keyof TOOLS & string;
    args: any;
  }>;
  
  // æ‰§è¡Œä¿¡æ¯
  finishReason: string;        // ç»“æŸåŸå› 
  usage: {                     // Token ç»Ÿè®¡
    inputTokens: number;
    outputTokens: number;
    totalTokens: number;
    reasoningTokens?: number;  // æ¨ç† tokens
  };
  
  // æ¨ç†ç›¸å…³
  reasoning?: Array<any>;      // æ¨ç†è¾“å‡º
  reasoningText?: string;      // æ¨ç†æ–‡æœ¬
  reasoningDetails?: Array<any>; // Anthropic æ¨ç†è¯¦æƒ…
}
```

---

## æœ€ä½³å®è·µ

### 1. ç¯å¢ƒå˜é‡ç®¡ç†

```typescript
// .env
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...

// ä½¿ç”¨
import 'dotenv/config';

const client = new LLMClient({
  provider: 'openai',
  apiKey: process.env.OPENAI_API_KEY,
  model: 'gpt-4',
});
```

### 2. Token ä½¿ç”¨ç›‘æ§

```typescript
const response = await client.chat(messages);

console.log(`Input tokens: ${response.usage.inputTokens}`);
console.log(`Output tokens: ${response.usage.outputTokens}`);
console.log(`Total tokens: ${response.usage.totalTokens}`);
```

### 3. å·¥å…·è°ƒç”¨å¾ªç¯ä¿æŠ¤

```typescript
let response = await client.chat(messages, { tools });
let iteration = 0;
const maxIterations = 10;

while (response.toolCalls && iteration < maxIterations) {
  iteration++;
  // å¤„ç†å·¥å…·è°ƒç”¨...
  response = await client.chat(messages, { tools });
}

if (iteration >= maxIterations) {
  console.warn('Max iterations reached');
}
```

---

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Ÿ

```typescript
import { createOpenAI } from '@ai-sdk/openai';

// Ollama
const ollama = createOpenAI({
  baseURL: 'http://localhost:11434/v1',
  apiKey: 'ollama',
});

const client = new LLMClient({
  languageModel: ollama('llama2'),
});
```

### Q: å¦‚ä½•å¤„ç†è¶…æ—¶ï¼Ÿ

```typescript
const controller = new AbortController();
setTimeout(() => controller.abort(), 30000);

try {
  const response = await client.chat(messages, {
    abortSignal: controller.signal,
  });
} catch (error) {
  if (error.name === 'AbortError') {
    console.error('Request timeout');
  }
}
```

---

## ç›¸å…³èµ„æº

- [Vercel AI SDK æ–‡æ¡£](https://sdk.vercel.ai/docs)
- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs)
- [Anthropic API æ–‡æ¡£](https://docs.anthropic.com)

---

## License

MIT
